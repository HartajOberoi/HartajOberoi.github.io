{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1 – Conceptual Foundations of RL and MDPs"
      ],
      "metadata": {
        "id": "DonLS6Pi91P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 1\n",
        "\n",
        "# (a) Definitions and Distinctions\n",
        "\n",
        "def explain_definitions():\n",
        "    definitions={\n",
        "        \"Discounted vs Average Reward MDPs\": \"Discounted MDPs consider future rewards with a discount factor gamma < 1, prioritizing short-term rewards. Average reward MDPs evaluate long-run average reward per step, suitable for ongoing tasks.\",\n",
        "        \"Model-Based vs Model-Free\": \"Model-Based methods (e.g., Policy Iteration) require knowledge of transition probabilities and rewards. Model-Free methods (e.g., Q-Learning) learn from interaction without an explicit model.\",\n",
        "        \"Q-Learning vs R-Learning\": \"Q-Learning is used for discounted MDPs to learn optimal Q-values. R-Learning is adapted for average-reward settings, estimating the average reward and relative action values.\"\n",
        "    }\n",
        "    return definitions\n",
        "\n",
        "def explain_unichain_importance():\n",
        "    return (\n",
        "        \"In average-reward MDPs, the unichain assumption (i.e., all policies induce Markov chains with a single recurrent class) ensures convergence of R-Learning and RVI. Without it, different parts of the state space might have distinct average rewards.\"\n",
        "    )\n",
        "\n",
        "# (b) DP Equations\n",
        "\n",
        "def discounted_dp_eq():\n",
        "    return \"Q*(x,a) = r(x,a) + gamma * sum_s' P(s'|x,a) * max_{a'} Q(s', a')\"\n",
        "\n",
        "def average_dp_eq():\n",
        "    return \"g* + h*(x) = max_a [ r(x,a) + sum P(s'|x,a) * h*(s') ]\"\n",
        "\n",
        "# (c) Algorithmic Steps\n",
        "\n",
        "def summarize_algorithms():\n",
        "    return {\n",
        "        \"RVI\": \"Relative Value Iteration subtracts the value of a reference state each iteration to ensure convergence in average-reward MDPs.\",\n",
        "        \"Policy Iteration\": \"Alternates between evaluating a policy and improving it using Bellman optimality.\",\n",
        "        \"LP for First Passage\": \"Formulates a linear program where constraints ensure the expected first passage reward is maximized/minimized.\"\n",
        "    }\n",
        "\n",
        "print(\"--- Problem 1 ---\")\n",
        "print(\"Definitions:\")\n",
        "for key, val in explain_definitions().items():\n",
        "    print(f\"{key}: {val}\")\n",
        "\n",
        "print(\"\\nUnichain Importance:\")\n",
        "print(explain_unichain_importance())\n",
        "\n",
        "print(\"\\nDP Equations:\")\n",
        "print(\"Discounted:\", discounted_dp_eq())\n",
        "print(\"Average:\", average_dp_eq())\n",
        "\n",
        "print(\"\\nAlgorithm Summaries:\")\n",
        "for key, val in summarize_algorithms().items():\n",
        "    print(f\"{key}: {val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPxlWpszBcfY",
        "outputId": "26d24377-4c77-4dfa-fb0e-104f5555e0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Problem 1 ---\n",
            "Definitions:\n",
            "Discounted vs Average Reward MDPs: Discounted MDPs consider future rewards with a discount factor gamma < 1, prioritizing short-term rewards. Average reward MDPs evaluate long-run average reward per step, suitable for ongoing tasks.\n",
            "Model-Based vs Model-Free: Model-Based methods (e.g., Policy Iteration) require knowledge of transition probabilities and rewards. Model-Free methods (e.g., Q-Learning) learn from interaction without an explicit model.\n",
            "Q-Learning vs R-Learning: Q-Learning is used for discounted MDPs to learn optimal Q-values. R-Learning is adapted for average-reward settings, estimating the average reward and relative action values.\n",
            "\n",
            "Unichain Importance:\n",
            "In average-reward MDPs, the unichain assumption (i.e., all policies induce Markov chains with a single recurrent class) ensures convergence of R-Learning and RVI. Without it, different parts of the state space might have distinct average rewards.\n",
            "\n",
            "DP Equations:\n",
            "Discounted: Q*(x,a) = r(x,a) + gamma * sum_s' P(s'|x,a) * max_{a'} Q(s', a')\n",
            "Average: g* + h*(x) = max_a [ r(x,a) + sum P(s'|x,a) * h*(s') ]\n",
            "\n",
            "Algorithm Summaries:\n",
            "RVI: Relative Value Iteration subtracts the value of a reference state each iteration to ensure convergence in average-reward MDPs.\n",
            "Policy Iteration: Alternates between evaluating a policy and improving it using Bellman optimality.\n",
            "LP for First Passage: Formulates a linear program where constraints ensure the expected first passage reward is maximized/minimized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Definitions\n",
        "Discounted vs. Average Reward MDPs:\n",
        "\n",
        "Discounted: Uses a discount factor γ ∈ (0,1) to prioritize immediate rewards.\n",
        "\n",
        "Average: Focuses on the long-run average reward per time step, ideal for continuing tasks.\n",
        "\n",
        "Model-Based vs. Model-Free:\n",
        "\n",
        "Model-Based: Assumes access to transition/reward models (e.g., Policy Iteration).\n",
        "\n",
        "Model-Free: Learns from experience without knowing the MDP dynamics (e.g., Q-learning).\n",
        "\n",
        "Q-Learning vs. R-Learning:\n",
        "\n",
        "Q-Learning: For discounted MDPs; estimates Q-values directly.\n",
        "\n",
        "R-Learning: Designed for average reward; estimates gain and relative Q-values.\n",
        "\n",
        "(b) Importance of Unichain Assumption\n",
        "Ensures that every policy leads to a Markov chain with a single recurrent class.\n",
        "\n",
        "This guarantees that algorithms like R-learning or Relative Value Iteration converge to the correct average reward.\n",
        "\n",
        "(c) DP Equations\n",
        "Discounted DP Equation:\n",
        "\n",
        "𝑄\n",
        "∗\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "=\n",
        "𝑟\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "+\n",
        "𝛾\n",
        "∑\n",
        "𝑠\n",
        "′\n",
        "𝑃\n",
        "(\n",
        "𝑠\n",
        "′\n",
        "∣\n",
        "𝑥\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "max\n",
        "⁡\n",
        "𝑎\n",
        "′\n",
        "𝑄\n",
        "(\n",
        "𝑠\n",
        "′\n",
        ",\n",
        "𝑎\n",
        "′\n",
        ")\n",
        "Q\n",
        "∗\n",
        " (x,a)=r(x,a)+γ\n",
        "s\n",
        "′\n",
        "\n",
        "∑\n",
        "​\n",
        " P(s\n",
        "′\n",
        " ∣x,a)\n",
        "a\n",
        "′\n",
        "\n",
        "max\n",
        "​\n",
        " Q(s\n",
        "′\n",
        " ,a\n",
        "′\n",
        " )\n",
        "Average DP Equation:\n",
        "\n",
        "𝑔\n",
        "∗\n",
        "+\n",
        "ℎ\n",
        "∗\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "max\n",
        "⁡\n",
        "𝑎\n",
        "[\n",
        "𝑟\n",
        "(\n",
        "𝑥\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "+\n",
        "∑\n",
        "𝑠\n",
        "′\n",
        "𝑃\n",
        "(\n",
        "𝑠\n",
        "′\n",
        "∣\n",
        "𝑥\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "ℎ\n",
        "∗\n",
        "(\n",
        "𝑠\n",
        "′\n",
        ")\n",
        "]\n",
        "g\n",
        "∗\n",
        " +h\n",
        "∗\n",
        " (x)=\n",
        "a\n",
        "max\n",
        "​\n",
        " [r(x,a)+\n",
        "s\n",
        "′\n",
        "\n",
        "∑\n",
        "​\n",
        " P(s\n",
        "′\n",
        " ∣x,a)h\n",
        "∗\n",
        " (s\n",
        "′\n",
        " )]\n",
        "(d) Algorithmic Summaries\n",
        "Relative Value Iteration (RVI): Subtracts a reference state’s value at each iteration to aid convergence in average-reward problems.\n",
        "\n",
        "Policy Iteration: Alternates between evaluating and improving a policy.\n",
        "\n",
        "LP for First Passage: Uses Linear Programming to find expected cost or reward of reaching a specific state."
      ],
      "metadata": {
        "id": "gwWBbw279Fpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 – Inventory Control MDP (Discounted)"
      ],
      "metadata": {
        "id": "Qwzix2C298Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem 2\n",
        "\n",
        "# (a) MDP Formulation\n",
        "\n",
        "states = [0, 1, 2]\n",
        "\n",
        "actions = [0, 1]\n",
        "\n",
        "demand_prob = {\n",
        "    0: 0.3,\n",
        "    1: 0.5,\n",
        "    2: 0.2\n",
        "}\n",
        "\n",
        "def next_state(s, a, d):\n",
        "    return max(0, min(2, s + a - d))\n",
        "\n",
        "# (b) Reward Function\n",
        "\n",
        "def reward(s, a):\n",
        "    expected_reward = 0\n",
        "    for d, prob in demand_prob.items():\n",
        "        inventory = s + a\n",
        "        sold = min(inventory, d)\n",
        "        end_inventory = inventory - sold\n",
        "        r = sold * 5 - end_inventory * 1\n",
        "        expected_reward += prob * r\n",
        "    return expected_reward\n",
        "\n",
        "# (c) Discounted or Average\n",
        "\n",
        "gamma = 0.95\n",
        "\n",
        "def transition_probs(s, a):\n",
        "    trans = {}\n",
        "    for d, prob in demand_prob.items():\n",
        "        s_prime = next_state(s, a, d)\n",
        "        trans[s_prime] = trans.get(s_prime, 0) + prob\n",
        "    return trans\n",
        "\n",
        "print(\"\\n--- Transition Probabilities ---\")\n",
        "for s in states:\n",
        "    for a in actions:\n",
        "        print(f\"From state {s}, action {a} =>\", transition_probs(s, a))\n",
        "\n",
        "# (d) Solution Steps\n",
        "\n",
        "policy = {s: 0 for s in states}\n",
        "V_pi = {s: 0 for s in states}\n",
        "\n",
        "for _ in range(1):\n",
        "    V_new = {}\n",
        "    for s in states:\n",
        "        a = policy[s]\n",
        "        val = 0\n",
        "        for d, prob in demand_prob.items():\n",
        "            s_prime = next_state(s, a, d)\n",
        "            val += prob * (reward(s, a) + gamma * V_pi[s_prime])\n",
        "        V_new[s] = val\n",
        "    V_pi = V_new\n",
        "\n",
        "print(\"\\n--- One Iteration of Policy Evaluation (Always order 0) ---\")\n",
        "print(V_pi)\n",
        "\n",
        "new_policy = {}\n",
        "for s in states:\n",
        "    best_a = None\n",
        "    best_val = float('-inf')\n",
        "    for a in actions:\n",
        "        val = 0\n",
        "        for d, prob in demand_prob.items():\n",
        "            s_prime = next_state(s, a, d)\n",
        "            val += prob * (reward(s, a) + gamma * V_pi[s_prime])\n",
        "        if val > best_val:\n",
        "            best_val = val\n",
        "            best_a = a\n",
        "    new_policy[s] = best_a\n",
        "\n",
        "print(\"\\n--- Improved Policy ---\")\n",
        "print(new_policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCYjBVJ9Bdk6",
        "outputId": "99dc9ba0-0027-4098-ba15-bb19fae7681c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Transition Probabilities ---\n",
            "From state 0, action 0 => {0: 1.0}\n",
            "From state 0, action 1 => {1: 0.3, 0: 0.7}\n",
            "From state 1, action 0 => {1: 0.3, 0: 0.7}\n",
            "From state 1, action 1 => {2: 0.3, 1: 0.5, 0: 0.2}\n",
            "From state 2, action 0 => {2: 0.3, 1: 0.5, 0: 0.2}\n",
            "From state 2, action 1 => {2: 0.8, 1: 0.2}\n",
            "\n",
            "--- One Iteration of Policy Evaluation (Always order 0) ---\n",
            "{0: 0.0, 1: 3.2, 2: 3.4}\n",
            "\n",
            "--- Improved Policy ---\n",
            "{0: 1, 1: 1, 2: 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) MDP Components\n",
        "States: Inventory levels {0, 1, 2}\n",
        "\n",
        "Actions: Order 0 or 1 item\n",
        "\n",
        "Demand Probabilities:\n",
        "\n",
        "Demand 0: 30%\n",
        "\n",
        "Demand 1: 50%\n",
        "\n",
        "Demand 2: 20%\n",
        "\n",
        "(b) Reward Function\n",
        "Revenue = $5 per item sold\n",
        "\n",
        "Holding cost = $1 per unsold item\n",
        "\n",
        "Expected reward is computed as:\n",
        "\n",
        "ExpectedReward\n",
        "(\n",
        "𝑠\n",
        ",\n",
        "𝑎\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑑\n",
        "𝑃\n",
        "(\n",
        "𝑑\n",
        ")\n",
        "⋅\n",
        "[\n",
        "min\n",
        "⁡\n",
        "(\n",
        "𝑠\n",
        "+\n",
        "𝑎\n",
        ",\n",
        "𝑑\n",
        ")\n",
        "⋅\n",
        "5\n",
        "−\n",
        "leftover\n",
        "⋅\n",
        "1\n",
        "]\n",
        "ExpectedReward(s,a)=\n",
        "d\n",
        "∑\n",
        "​\n",
        " P(d)⋅[min(s+a,d)⋅5−leftover⋅1]\n",
        "(c) Transition Probabilities\n",
        "Given state and action, demand is sampled → determines next state.\n",
        "\n",
        "(d) Value Iteration / Policy Iteration\n",
        "Starts with a fixed policy (always order 0).\n",
        "\n",
        "Performs one step of policy evaluation.\n",
        "\n",
        "Then policy improvement to update decisions at each state."
      ],
      "metadata": {
        "id": "A8Wyaj7t-Ozw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4 – Application Scenarios of RL"
      ],
      "metadata": {
        "id": "5SjKPeFw-Zdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 4\n",
        "\n",
        "# (a) AI / Robotics\n",
        "print(\"\\n--- AI / Robotics ---\")\n",
        "ai_states = [\"x0\", \"x1\", \"x2\"]\n",
        "ai_actions = [\"Forward\", \"Turn\"]\n",
        "ai_transitions = {\n",
        "    (\"x0\", \"Forward\"): [(\"x1\", 0.8), (\"x0\", 0.2)],\n",
        "    (\"x0\", \"Turn\"): [(\"x0\", 1.0)],\n",
        "    (\"x1\", \"Forward\"): [(\"x2\", 0.9), (\"x1\", 0.1)],\n",
        "    (\"x1\", \"Turn\"): [(\"x0\", 0.7), (\"x1\", 0.3)],\n",
        "    (\"x2\", \"Forward\"): [(\"x2\", 1.0)],\n",
        "    (\"x2\", \"Turn\"): [(\"x1\", 0.6), (\"x2\", 0.4)]\n",
        "}\n",
        "ai_rewards = {\n",
        "    (\"x0\", \"Forward\"): 1,\n",
        "    (\"x0\", \"Turn\"): 0,\n",
        "    (\"x1\", \"Forward\"): 10,\n",
        "    (\"x1\", \"Turn\"): -1,\n",
        "    (\"x2\", \"Forward\"): -5,\n",
        "    (\"x2\", \"Turn\"): 0\n",
        "}\n",
        "\n",
        "print(\"States:\", ai_states)\n",
        "print(\"Actions:\", ai_actions)\n",
        "print(\"Example Reward (x1, Forward):\", ai_rewards[(\"x1\", \"Forward\")])\n",
        "\n",
        "# (b) Security / Cyber\n",
        "print(\"\\n--- Security / Cyber ---\")\n",
        "sec_states = [\"Secure\", \"Suspicious\", \"Compromised\"]\n",
        "sec_actions = [\"Scan\", \"DoNothing\"]\n",
        "sec_transitions = {\n",
        "    (\"Secure\", \"Scan\"): [(\"Secure\", 0.95), (\"Suspicious\", 0.05)],\n",
        "    (\"Secure\", \"DoNothing\"): [(\"Secure\", 0.8), (\"Suspicious\", 0.2)],\n",
        "    (\"Suspicious\", \"Scan\"): [(\"Secure\", 0.5), (\"Compromised\", 0.5)],\n",
        "    (\"Suspicious\", \"DoNothing\"): [(\"Compromised\", 0.7), (\"Suspicious\", 0.3)],\n",
        "    (\"Compromised\", \"Scan\"): [(\"Suspicious\", 0.6), (\"Compromised\", 0.4)],\n",
        "    (\"Compromised\", \"DoNothing\"): [(\"Compromised\", 1.0)]\n",
        "}\n",
        "sec_rewards = {\n",
        "    (\"Secure\", \"Scan\"): -1,\n",
        "    (\"Secure\", \"DoNothing\"): 0,\n",
        "    (\"Suspicious\", \"Scan\"): -2,\n",
        "    (\"Suspicious\", \"DoNothing\"): -5,\n",
        "    (\"Compromised\", \"Scan\"): -10,\n",
        "    (\"Compromised\", \"DoNothing\"): -20\n",
        "}\n",
        "\n",
        "print(\"States:\", sec_states)\n",
        "print(\"Actions:\", sec_actions)\n",
        "print(\"Example Reward (Suspicious, Scan):\", sec_rewards[(\"Suspicious\", \"Scan\")])\n",
        "\n",
        "# (c) Healthcare\n",
        "print(\"\\n--- Healthcare ---\")\n",
        "health_states = [\"Stable\", \"SlightlyWorse\", \"Critical\"]\n",
        "health_actions = [\"Monitor\", \"Treat\"]\n",
        "health_transitions = {\n",
        "    (\"Stable\", \"Monitor\"): [(\"Stable\", 0.7), (\"SlightlyWorse\", 0.3)],\n",
        "    (\"Stable\", \"Treat\"): [(\"Stable\", 0.9), (\"SlightlyWorse\", 0.1)],\n",
        "    (\"SlightlyWorse\", \"Monitor\"): [(\"Stable\", 0.2), (\"Critical\", 0.8)],\n",
        "    (\"SlightlyWorse\", \"Treat\"): [(\"Stable\", 0.6), (\"SlightlyWorse\", 0.4)],\n",
        "    (\"Critical\", \"Monitor\"): [(\"Critical\", 1.0)],\n",
        "    (\"Critical\", \"Treat\"): [(\"SlightlyWorse\", 0.7), (\"Critical\", 0.3)]\n",
        "}\n",
        "health_rewards = {\n",
        "    (\"Stable\", \"Monitor\"): 5,\n",
        "    (\"Stable\", \"Treat\"): 4,\n",
        "    (\"SlightlyWorse\", \"Monitor\"): -2,\n",
        "    (\"SlightlyWorse\", \"Treat\"): 2,\n",
        "    (\"Critical\", \"Monitor\"): -10,\n",
        "    (\"Critical\", \"Treat\"): -1\n",
        "}\n",
        "\n",
        "print(\"States:\", health_states)\n",
        "print(\"Actions:\", health_actions)\n",
        "print(\"Example Reward (Critical, Treat):\", health_rewards[(\"Critical\", \"Treat\")])\n",
        "\n",
        "print(\"\\n--- RL Method ---\")\n",
        "print(\"Q-Learning or R-Learning can be used where transition probabilities are unknown. Use ε-greedy exploration to balance exploration/exploitation.\")\n",
        "\n",
        "print(\"\\n--- Simulation Design ---\")\n",
        "print(\"Start from a middle state. Simulate episodes with ε-greedy policy. Update Q-values using observed rewards and next states. Evaluate final policy by comparing total cumulative reward over multiple runs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOh25AmW_QOO",
        "outputId": "526c96ba-3471-4024-b6b5-c85f08d076ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- AI / Robotics ---\n",
            "States: ['x0', 'x1', 'x2']\n",
            "Actions: ['Forward', 'Turn']\n",
            "Example Reward (x1, Forward): 10\n",
            "\n",
            "--- Security / Cyber ---\n",
            "States: ['Secure', 'Suspicious', 'Compromised']\n",
            "Actions: ['Scan', 'DoNothing']\n",
            "Example Reward (Suspicious, Scan): -2\n",
            "\n",
            "--- Healthcare ---\n",
            "States: ['Stable', 'SlightlyWorse', 'Critical']\n",
            "Actions: ['Monitor', 'Treat']\n",
            "Example Reward (Critical, Treat): -1\n",
            "\n",
            "--- RL Method ---\n",
            "Q-Learning or R-Learning can be used where transition probabilities are unknown. Use ε-greedy exploration to balance exploration/exploitation.\n",
            "\n",
            "--- Simulation Design ---\n",
            "Start from a middle state. Simulate episodes with ε-greedy policy. Update Q-values using observed rewards and next states. Evaluate final policy by comparing total cumulative reward over multiple runs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) AI / Robotics\n",
        "States: Positions (x0, x1, x2)\n",
        "\n",
        "Actions: Forward, Turn\n",
        "\n",
        "Transition Probabilities: Stochastic; e.g., 80% chance of moving forward from x0 to x1.\n",
        "\n",
        "Reward Structure:\n",
        "\n",
        "Positive for progress (e.g., +10 for x1 Forward)\n",
        "\n",
        "Negative for inefficiency (e.g., -5 for x2 Forward)\n",
        "\n",
        "(b) Cybersecurity\n",
        "States: Secure → Suspicious → Compromised\n",
        "\n",
        "Actions: Scan, DoNothing\n",
        "\n",
        "Rewards penalize unsafe states or costly scans.\n",
        "\n",
        "(c) Healthcare Monitoring\n",
        "States: Stable → Slightly Worse → Critical\n",
        "\n",
        "Actions: Monitor, Treat\n",
        "\n",
        "Rewards encourage stabilizing the patient, penalize deterioration.\n",
        "\n",
        "(d) RL Approach\n",
        "Use Q-learning or R-learning depending on reward structure (discounted vs. average).\n",
        "\n",
        "Apply ε-greedy exploration to balance learning and exploiting.\n",
        "\n",
        "Simulation-based training: Start from a middle state, simulate episodes, update Q-values, and evaluate policy performance."
      ],
      "metadata": {
        "id": "XIzsmrJR-cjt"
      }
    }
  ]
}